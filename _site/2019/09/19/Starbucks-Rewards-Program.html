<figure class="">
  <img src="/assets/images/posts/starbucks-daiga-ellaby-unsplash-1000.jpg" alt="Starbucks cup of coffee and mobile" />
  
    <figcaption>
      

    </figcaption></figure>

<p>Photo by <a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &quot;San Francisco&quot;, &quot;Helvetica Neue&quot;, Helvetica, Ubuntu, Roboto, Noto, &quot;Segoe UI&quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px" href="https://unsplash.com/@daiga_ellaby?utm_medium=referral&amp;utm_campaign=photographer-credit&amp;utm_content=creditBadge" target="_blank" rel="noopener noreferrer" title="Download free do whatever you want high-resolution photos from Daiga Ellaby"><span style="display:inline-block;padding:2px 3px"><svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-2px;fill:white" viewBox="0 0 32 32"><title>unsplash-logo</title><path d="M10 9V0h12v9H10zm12 5h10v18H0V14h10v9h12v-9z"></path></svg></span><span style="display:inline-block;padding:2px 3px">Daiga Ellaby</span></a></p>

<p class="notice--primary"><em>This is the technical report I wrote for the capstone project in Udacity Data Science Nanodegree. The code for this project can be found on my GitHub <a href="https://github.com/k-bosko/Starbucks_rewards">here</a>.</em></p>

<h1 id="1-project-definition">1. Project Definition</h1>
<h2 id="11-project-overview">1.1. Project Overview</h2>

<p>In this project I analyze the customer behavior in the Starbucks rewards mobile app.<sup id="fnref:ft1" role="doc-noteref"><a href="#fn:ft1" class="footnote" rel="footnote">1</a></sup> After signing up for the app, customers receive promotions every few days. The task is to identify which customers are influenced by promotional offers the most and what types of offers to send them in order to maximize the revenue.</p>

<p>There are three types of promotions:</p>
<ul>
  <li>discount</li>
  <li>bogo (buy one, get one free)</li>
  <li>informational - product advertisement without any price off</li>
</ul>

<p>Each offer is valid for certain number of days before it expires. Discounts and bogos have also different difficulty level, depending on how much the customer has to spend in order to earn the promotion. Promotions are distributed via different multiple channels (social, web, email, mobile).</p>

<p>All transactions made through the app are tracked automatically. The app also records information about which offers have been sent, which have been viewed and which have been completed and when these three events happened.</p>

<h2 id="12-dataset-overview">1.2. Dataset Overview</h2>
<p>The data is organized in three files:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">portfolio.json</code> (10 offers x 6 fields) - offer types sent during 30-day test period</li>
  <li><code class="language-plaintext highlighter-rouge">profile.json</code> (17000 users x 5 fields) - demographic profile of app users</li>
  <li><code class="language-plaintext highlighter-rouge">transcript.json</code> (306648 events x 4 fields) - event log on transactions, offers received, offers viewed, and offers completed</li>
</ul>

<p>Here is the schema and explanation of each variable in the files:</p>

<p><strong>portfolio.json</strong></p>

<ul>
  <li>id (string) - offer id</li>
  <li>offer_type (string) - type of offer ie BOGO, discount, informational</li>
  <li>difficulty (int) - minimum required spend to complete an offer</li>
  <li>reward (int) - reward given for completing an offer</li>
  <li>duration (int) - time for offer to be open, in days</li>
  <li>channels (list of strings)</li>
</ul>

<p><strong>profile.json</strong></p>
<ul>
  <li>age (int) - age of the customer</li>
  <li>became_member_on (int) - date when customer created an app account</li>
  <li>gender (str) - gender of the customer</li>
  <li>id (str) - customer id</li>
  <li>income (float) - customer’s income</li>
</ul>

<p><strong>transcript.json</strong></p>
<ul>
  <li>event (str) - record description (ie transaction, offer received, offer viewed, etc.)</li>
  <li>person (str) - customer id</li>
  <li>time (int) - time in hours since start of test. The data begins at time t=0</li>
  <li>value - (dict of strings) - either an offer id or transaction amount depending on the record</li>
</ul>

<h2 id="13-problem-statement">1.3. Problem Statement</h2>

<p>The aim of this project is to identify target audience for a successful marketing campaign in direct marketing:</p>

<blockquote>
  <p>Direct marketing describes the marketer’s efforts to directly reach the customer through direct marketing communications (direct mail, e-mail, social media, and similar “personalized” or one-on-one means). The direct marketiing effort requires marketers to have a target list of customers that will each receive a marketing message tailored to their needs and interests. — John A. Davis: “Measuring Marketing”, 3d Edition, 2018, Part 8.</p>
</blockquote>

<p>To solve this task, I performed customer segmentation using <strong>K-means clustering technique</strong>.</p>

<p>The idea is to divide app users into major groups - those more prone to discounts vs. those more keen on bogos vs. those that are not interested in promotions at all. The number of groups was decided at the later stage depending on the actual patterns in the final dataset.</p>

<h2 id="14-metrics">1.4. Metrics</h2>

<p>The critical decision in customer segmentation task is to choose the optimal number of segments. The problem  with unsupervised machine learning is that it doesn’t have clearly defined benchmark metrics for model performance evaluation on par with supervised ML (e.g. accuracy score, f1-score, AUC, etc.). Instead there is a number of heuristics that aid analysts during decision-making process, but which ultimately don’t say anything whether the modeling results are good or bad. For k-means clustering, these heuristics are elbow curve method and silouhette score for deciding upon optimal number of clusters (see section 5.1. for more details).</p>

<p>To evaluate the segmentation results, I relied on the following marketing metrics<sup id="fnref:ft2" role="doc-noteref"><a href="#fn:ft2" class="footnote" rel="footnote">2</a></sup>:</p>
<ul>
  <li><strong>Response Rate (RR)</strong> - the percentage of customers who viewed an offer relative to the number of customers that received the offer</li>
  <li><strong>Conversion Rate (CVR)</strong> - the percentage of customers who completed an offer relative to the number of customers who viewed an offer</li>
</ul>

<p>These two marketing metrics help marketers improve efficiency and reduce costs of a marketing campaign. The response rate tells how many customers are interested in offers, while the conversion rates show whether the offers sent were attractive enough to complete them.</p>

<p>By calculating each customer’s RR and CVR for all bogos, discounts and informational offers sent to him/her, I was able to evaluate the resulting segments in alignment with the project’s task, i.e. identify target audience for each offer type.</p>

<h1 id="2-data-munging">2. Data Munging</h1>

<h2 id="21-data-cleaning">2.1. Data Cleaning</h2>

<p>Similar to most data science tasks, the data cleaning and munging consumed the most amount of time when I worked on this project.</p>

<p>The first step was to combine the three datasets (portfolio, profile, transactions) into one final dataset. Because the project’s goal was to identify the customer segments based on their engagement with promotional offers, I decided to aggregate data at the level of each customer. However, before that was possible I needed to reorganize transactions data which was rather messy. In the following I describe the major challenge confronted here.</p>

<p><strong>Correcting Offers Completed and Offers Viewed</strong></p>

<p>The particular challenge with event log data was that it didn’t take into account the particular sequence of user’s interactions with offers. Thus, the app marked offers as completed when they satisfied offer criteria (expiration date and amount to be spent). This means that users might have earned rewards even when they didn’t view an offer. Similarly, the offers viewed didn’t take timing into consideration, which resulted into a situation when offer was counted as viewed even if a user viewed an offer after it expired. As a result, the completed and viewed offers as they were recorded in the event log didn’t reflect the actual campaign’s effect and the overall campaign results were distorted.</p>

<p>To address this problem, I identified which offers were viewed during the offer time window (before expiration date) and which offers were completed after viewing and before expiration date. These were separated from offers that were incorrectly attributed as completed or viewed. Both CVR and RR metrics reported here are based on the corrected values.</p>

<p>The technical side of this step can be summarized as follows.</p>

<p><strong>Create unique identifier for each offer sent:</strong></p>

<p>To evaluate which offers were viewed and completed correctly, I had to organize data at the offer level with timestamps for each event (received, viewed, completed). However, some customers received the same offer type more than once, meaning that the offer id is not a unique identifier in this case. When aggregating this data at later stage, information on these offers will be lost unless we create unique identifiers for them. To get an idea, there are 76277 offers sent, out of which 51570 offers were sent to a certain customer only once, while 24707 offers of the same types were sent more than once.</p>

<p>So I created a unique identfier for each offer by combining <code class="language-plaintext highlighter-rouge">groupby()</code> and <code class="language-plaintext highlighter-rouge">cumcount()</code> functions to count how many times the same offer type was sent to the same person and then combine this counter with the offer id into a unique identifier:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#create unique identifier for each offer sent (because same offers could be sent more than once)
</span>  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'id_unique_received'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="n">starbucks_time</span><span class="p">.</span><span class="n">offer_received</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">groupby</span><span class="p">([</span><span class="s">'person'</span><span class="p">,</span> <span class="s">'offer_id'</span><span class="p">]).</span><span class="n">cumcount</span><span class="p">()</span>
  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'id_unique_viewed'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="n">starbucks_time</span><span class="p">.</span><span class="n">offer_viewed</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">groupby</span><span class="p">([</span><span class="s">'person'</span><span class="p">,</span> <span class="s">'offer_id'</span><span class="p">]).</span><span class="n">cumcount</span><span class="p">()</span>
  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'id_unique_completed'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="n">starbucks_time</span><span class="p">.</span><span class="n">offer_completed</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">groupby</span><span class="p">([</span><span class="s">'person'</span><span class="p">,</span> <span class="s">'offer_id'</span><span class="p">]).</span><span class="n">cumcount</span><span class="p">()</span>
  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'id_unique_events'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[[</span><span class="s">'id_unique_received'</span><span class="p">,</span> <span class="s">'id_unique_viewed'</span><span class="p">,</span> <span class="s">'id_unique_completed'</span><span class="p">]].</span><span class="nb">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span>
  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'id_unique'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_id'</span><span class="p">]</span> <span class="o">+</span> <span class="s">"-"</span> <span class="o">+</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'id_unique_events'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div></div>
<p>For instance, if the offer type <code class="language-plaintext highlighter-rouge">'fafdcd668e3743c1bb461111dcafc2a4'</code> was sent to the same person twice, the first offer sent gets unique id as <code class="language-plaintext highlighter-rouge">'fafdcd668e3743c1bb461111dcafc2a4-0.0'</code> while the second offer of the same type gets unique id of <code class="language-plaintext highlighter-rouge">'fafdcd668e3743c1bb461111dcafc2a4-1.0'</code>.</p>

<p> </p>

<p><strong>Pivot timestamps data from long to wide format</strong></p>

<p>Since the data in transactions dataset was in the long format, the next challenge was to organize it into wide format with combination of <code class="language-plaintext highlighter-rouge">groupby()</code>, <code class="language-plaintext highlighter-rouge">max()</code> and <code class="language-plaintext highlighter-rouge">unstack()</code> functions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># create columns with time for each event
</span>  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_received_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_received'</span><span class="p">]</span><span class="o">*</span><span class="n">starbucks_time</span><span class="p">.</span><span class="n">time</span>
  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_viewed_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_viewed'</span><span class="p">]</span><span class="o">*</span><span class="n">starbucks_time</span><span class="p">.</span><span class="n">time</span>
  <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_completed_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[</span><span class="s">'offer_completed'</span><span class="p">]</span><span class="o">*</span><span class="n">starbucks_time</span><span class="p">.</span><span class="n">time</span>
  <span class="n">starbucks_time</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">[[</span><span class="s">'person'</span><span class="p">,</span> <span class="s">'id_unique'</span><span class="p">,</span> <span class="s">'offer_id'</span><span class="p">,</span> <span class="s">'time'</span><span class="p">,</span> <span class="s">'offer_received_time'</span><span class="p">,</span> <span class="s">'offer_viewed_time'</span><span class="p">,</span> <span class="s">'offer_completed_time'</span><span class="p">]]</span>

  <span class="c1"># unstack values to get to the level of each (person, offer id) tuple
</span>  <span class="c1"># need to take max values to avoid 0s
</span>  <span class="n">starbucks_time_full</span> <span class="o">=</span> <span class="n">starbucks_time</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s">'person'</span><span class="p">,</span> <span class="s">'id_unique'</span><span class="p">,</span> <span class="s">'time'</span><span class="p">]).</span><span class="nb">max</span><span class="p">().</span><span class="n">unstack</span><span class="p">()</span>
  <span class="n">starbucks_time_full</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

  <span class="c1"># create the final time-based dataset for each offer sent
</span>  <span class="n">offers</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">starbucks_time_full</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">get_level_values</span><span class="p">(</span><span class="s">'id_unique'</span><span class="p">),</span> <span class="n">starbucks_time_full</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">get_level_values</span><span class="p">(</span><span class="s">'person'</span><span class="p">)).</span><span class="n">reset_index</span><span class="p">()</span>
  <span class="n">offers</span><span class="p">[</span><span class="s">'offer_received_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time_full</span><span class="p">[</span><span class="s">'offer_received_time'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">offers</span><span class="p">[</span><span class="s">'offer_viewed_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time_full</span><span class="p">[</span><span class="s">'offer_viewed_time'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">offers</span><span class="p">[</span><span class="s">'offer_completed_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">starbucks_time_full</span><span class="p">[</span><span class="s">'offer_completed_time'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>
<p>So from this:</p>
<figure class="">
  <img src="/assets/images/posts/starbucks-timestamps-original.png" alt="Timestamp records of each event" />
  </figure>

<p>… we got to this:</p>
<figure class="">
  <img src="/assets/images/posts/starbucks-timestamps-organized.png" alt="Timestamps of each event organized at offer level" />
  </figure>

<p> 
<strong>Add offer end time</strong></p>

<p>Then I combined the time-based data with portfolio data to access the duration of each offer and calculated the offer expiration timestamp by adding the duration time (converted from days to hours) to received timestamp in hours.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># add information about each offer from portfolio
</span>  <span class="n">offers</span><span class="p">[</span><span class="s">'id'</span><span class="p">]</span> <span class="o">=</span> <span class="n">offers</span><span class="p">.</span><span class="n">id_unique</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">"-"</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">offers</span> <span class="o">=</span> <span class="n">offers</span><span class="p">.</span><span class="n">merge</span><span class="p">(</span><span class="n">portfolio</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'id'</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">'left'</span><span class="p">)</span>

  <span class="c1"># add offer end time (convert duration time that is in days to hours)
</span>  <span class="n">offers</span><span class="p">[</span><span class="s">'offer_end_time'</span><span class="p">]</span> <span class="o">=</span> <span class="n">offers</span><span class="p">[</span><span class="s">'offer_received_time'</span><span class="p">]</span><span class="o">+</span><span class="n">offers</span><span class="p">[</span><span class="s">'duration'</span><span class="p">].</span><span class="n">values</span><span class="o">*</span><span class="mi">24</span>
</code></pre></div></div>
<p> </p>

<p><strong>Identify completed offers and viewed offers correctly</strong></p>

<p>Now I had all the needed information on offer level to correctly attribute offers as completed or viewed.</p>

<p>For offer to be counted as <em>“viewed”</em> it had to be viewed before offer expiration date.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offers</span><span class="p">[</span><span class="s">'viewed_binary'</span><span class="p">]</span> <span class="o">=</span> <span class="n">offers</span><span class="p">.</span><span class="n">offer_viewed_time</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">offers</span><span class="p">[</span><span class="s">'viewed_on_time'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">offers</span><span class="p">.</span><span class="n">offer_viewed_time</span> <span class="o">&lt;</span> <span class="n">offers</span><span class="p">.</span><span class="n">offer_end_time</span><span class="p">)</span><span class="o">*</span><span class="n">offers</span><span class="p">[</span><span class="s">'viewed_binary'</span><span class="p">]</span> 
</code></pre></div></div>

<p>For offer to be <em>“completed”</em> it had to meet these conditions:</p>
<ol>
  <li>be completed before offer expiration date</li>
  <li>completed after viewing</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">offers</span><span class="p">[</span><span class="s">'completed_binary'</span><span class="p">]</span> <span class="o">=</span> <span class="n">offers</span><span class="p">.</span><span class="n">offer_completed_time</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">offers</span><span class="p">[</span><span class="s">'completed_on_time'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">offers</span><span class="p">.</span><span class="n">offer_completed_time</span> <span class="o">&lt;</span> <span class="n">offers</span><span class="p">.</span><span class="n">offer_end_time</span><span class="p">)</span><span class="o">*</span><span class="n">offers</span><span class="p">[</span><span class="s">'completed_binary'</span><span class="p">]</span> 

<span class="n">completed_before_expires</span> <span class="o">=</span> <span class="p">(</span><span class="n">offers</span><span class="p">.</span><span class="n">offer_completed_time</span> <span class="o">&lt;</span> <span class="n">offers</span><span class="p">.</span><span class="n">offer_end_time</span><span class="p">)</span>
<span class="n">completed_after_viewing</span> <span class="o">=</span><span class="p">(</span><span class="n">offers</span><span class="p">.</span><span class="n">offer_completed_time</span> <span class="o">&gt;</span> <span class="n">offers</span><span class="p">.</span><span class="n">offer_viewed_time</span><span class="p">)</span><span class="o">*</span><span class="n">offers</span><span class="p">[</span><span class="s">'viewed_binary'</span><span class="p">]</span>
<span class="n">offers</span><span class="p">[</span><span class="s">'completed_after_viewing'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">completed_after_viewing</span> <span class="o">&amp;</span> <span class="n">completed_before_expires</span><span class="p">)</span><span class="o">*</span><span class="n">offers</span><span class="p">[</span><span class="s">'completed_binary'</span><span class="p">]</span>
</code></pre></div></div>
<p>The effect of this “cleanup” is not to be underestimated. According to my analysis, <strong>Starbucks could have saved almost $70,000</strong> if it tracked completed offers correctly:</p>

<figure class="">
  <img src="/assets/images/posts/starbucks-amount-wasted.jpg" alt="Analysis results on rewarded total and amount waisted due to incorrect attribution of completed offers" />
  </figure>

<h2 id="22-feature-engineering">2.2. Feature Engineering</h2>

<p>The final dataset has 32 features, most of which were calculated or engineered. Thus, I created the following features:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">total_amount</code> (int): the amount spent by each customer during the 30 day test period</li>
  <li><code class="language-plaintext highlighter-rouge">total_rewarded</code> (int): the amount each customer was rewarded (Note: not the recorded sum, but correctly attributed sum)</li>
  <li><code class="language-plaintext highlighter-rouge">transactions_num</code> (int): number of transactions each customer made during the test period</li>
  <li><code class="language-plaintext highlighter-rouge">offers_received</code> (int): total number of offers received by each customer</li>
  <li><code class="language-plaintext highlighter-rouge">offers_viewed</code> (int): total number of offers viewed by each customer</li>
  <li><code class="language-plaintext highlighter-rouge">offers_completed</code> (int): total number of offers completed by each customer</li>
  <li><code class="language-plaintext highlighter-rouge">bogo_received</code>, <code class="language-plaintext highlighter-rouge">bogo_viewed</code>, <code class="language-plaintext highlighter-rouge">bogo_completed</code> (int): the number of bogo offers correspondingly received, viewed and completed</li>
  <li><code class="language-plaintext highlighter-rouge">discount_received</code>, <code class="language-plaintext highlighter-rouge">discount_viewed</code>, <code class="language-plaintext highlighter-rouge">discount_completed</code> (int): the number of discount offers correspondingly received, viewed and completed</li>
  <li><code class="language-plaintext highlighter-rouge">informational_received</code>, <code class="language-plaintext highlighter-rouge">informational_viewed</code> (int): the number of informational offers correspondingly received and viewed</li>
  <li><code class="language-plaintext highlighter-rouge">total_bogo</code>, <code class="language-plaintext highlighter-rouge">total_discount</code> (int): the amount each customer got rewarded correspondingly on bogo and discount offers</li>
  <li><code class="language-plaintext highlighter-rouge">avg_order_size</code> (int): average order size as total amount divided by number of transactions</li>
  <li><code class="language-plaintext highlighter-rouge">avg_reward_size</code> (int): average reward size as total amount rewarded divided by the number of discounts and bogo offers completed</li>
  <li><code class="language-plaintext highlighter-rouge">avg_bogo_size</code>, <code class="language-plaintext highlighter-rouge">avg_discount_size</code> (int): average bogo size as the number of total bogo amount rewarded divided by total number of bogo completed and the corresponding numbers for average discount size for each customer</li>
  <li><code class="language-plaintext highlighter-rouge">offers_rr</code>, <code class="language-plaintext highlighter-rouge">bogo_rr</code>, <code class="language-plaintext highlighter-rouge">discount_rr</code>, <code class="language-plaintext highlighter-rouge">informational_rr</code> (int): response rates of each customer at the overall offer level, bogo, discount and informational level</li>
  <li><code class="language-plaintext highlighter-rouge">offers_cvr</code>, <code class="language-plaintext highlighter-rouge">bogo_cvr</code>, <code class="language-plaintext highlighter-rouge">discount_cvr</code> (int): conversion rates of each customer at the general offer level and at the bogo and discount levels.</li>
</ul>

<p class="notice--info"><i class="far fa-sticky-note"></i> <strong>Note:</strong> All features that have to deal with offer viewing or completing reflect not recorded number, but correctly attributed number and the amounts that are based on these numbers reflect the correctly attributed sums, not the recorded sums (see 2.1. for more information). This ensures that we measure customer interaction with offers correctly.</p>

<h2 id="23-imputing-missing-values">2.3. Imputing Missing Values</h2>

<p>It turned out that there are about 2175 customers (12.8%) with missing values in the profile dataset. Rows with missing data had no information on customer’s age, income and gender simultaneously. I implemented two strategies to deal with missing data in a function:</p>
<ul>
  <li>drop the missing values all together (which is worse, since 12% is a lot of data)</li>
  <li>impute with median values for numerical columns (age and income) or impute the mode for categorical column (gender). The median values for age and income turned out to be <em>55 years</em> and <em>$64000</em> correspondingly and <em>Male</em> as the most frequent value for gender.</li>
</ul>

<h1 id="3-analysis">3. Analysis</h1>

<h2 id="31-data-exploration">3.1. Data Exploration</h2>

<p><strong>Customers</strong>:
The typical Stabucks mobile app customer is middle-aged (median - 55 years) and has income of about $64000). During the experiment, the customers spent on average $104.44 (min - $0, max - $1608.7) and got $5.6 (min - $0, max - $55) rewarded. Furthermore, customers would make on average 8 transactions with the average order size being $13.34.</p>

<p><strong>Offers</strong>: 
Customers would receive on average 4-5 offers, view about 3 offers and complete about 1 offer. The average return on bogo offers is higher than on discount offers - $3.8 vs $1.7. Each customer viewed on average 73% of offers he/she received (Response Rate), with bogo offers being viewed more often (73%) than discounts (61%) or informational offers (39%). Each customer reacted upon (CVR) about 34% of offers he/she viewed, with CVR for discounts being higher (38%) than for bogo (29%).</p>

<p><strong>Customer-Offer Interactions</strong>:
The original datasets contain information on 17000 customers, all with unique anonymized ids. During data exploration phase, it turned out that 16994 customers received offers, while 6 did not. Out of those who received, 16735 customers viewed offers, while 259 did not. Out of those who received &amp; viewed, 10640 customers completed the offers, while 6095 did not. As a result, Viewing Rate is quite high - 98.4%, while the overall completion rate is much lower - 62.6%.</p>

<p> 
 </p>

<h2 id="32-data-visualization">3.2. Data Visualization</h2>

<p><strong>Customer Profile</strong></p>

<figure class="">
  <img src="/assets/images/posts/starbucks-eda-profile.png" alt="Starbucks Customer Profile" />
  </figure>

<p><strong>Number of Events: Received, Viewed, Completed</strong></p>

<p>As noted in section 2.1., I reevaluated the number of events after imposing certain conditions. One can see from the plot below that the actual viewing and completion rate is much less:</p>

<p> </p>
<figure style="width: 80%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-eda-offer-events.png" alt="Number of Starbucks Promotional Events" />
</figure>

<p>By offer type these corrected values look like this:</p>

<figure class="">
  <img src="/assets/images/posts/starbucks-eda-offer-events2.png" alt="Number of Starbucks Promotional Events after Correction" />
  </figure>

<p>I also calculated correlations of certain numerical features with the metrics of interest:</p>

<figure style="width: 80%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-eda-correlations.png" alt="Correlation Matrix" />
</figure>

<p>From the correlation matrix, it seems like not the profile features, but rather spending habits (like number of transactions and total amount) seem to correlate more with the conversion rates. For bogo_cvr total amount spent seems to be more defining, while for discount_cvr - transaction frequency.</p>

<p> 
 </p>

<h1 id="4-data-preprocessing">4. Data Preprocessing</h1>

<p>The linear unsupervised machine learning and K-means clustering in particular rely on distances as measure of similarity and hence are prone to data scaling. To account for this problem, I further transformed the data before clustering - one-hot encoded categorical features, scaled the features and perform dimensionality reduction.</p>

<h2 id="41-one-hot-encoding">4.1. One Hot Encoding</h2>

<p>The majority of features in the final dataset are numerical except for <code class="language-plaintext highlighter-rouge">gender</code>, which is categorical. This column had to be one-hot encoded, meaning its values had to be converted into binary (dummy) variables that take on either 0 or 1 . Earlier in the project I also one-hot encoded offer types.</p>

<p>Besides that I decided to transform the original <code class="language-plaintext highlighter-rouge">became_member_on</code> into categorical column with membership years as its values. The categorical membership column now could serve as a proxy for customers loyalty and was easier to interpret afterwards. Because there are only 6 possible years (2013-2018), the one-hot encoding of this feature didn’t increase the data dimensionality that much.</p>

<h2 id="42-feature-scaling">4.2. Feature Scaling</h2>

<p>Next, the data was transformed to achieve the same scale needed for PCA and K-means clustering. This is an important step, as otherwise features with large scale would dominate the clustering process. I implemented two popular scaling techniques available in scikit-learn library (but found that standardization produced better results - see more in 4.5.)):</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">StandardScaler</code>: standardizes/normalizes the data by substracting the mean and dividing by the standard deviation;</li>
  <li><code class="language-plaintext highlighter-rouge">MinMaxScaler</code>: transforms the data so that all of its values are between zero and one.</li>
</ul>

<h2 id="43-dimensionality-reduction-with-pca">4.3. Dimensionality Reduction with PCA</h2>

<p>Dimensionality reduction helps reduce the noise in the data by projecting it from high-dimensional into low-dimensional space, while retaining as much of the variation as possible. The ML algorithms thus can identify patterns in the data more effectively (because of less variability in data) and more efficiently (because of less dimensions hence less computational power needed to make calculations).<sup id="fnref:ft3" role="doc-noteref"><a href="#fn:ft3" class="footnote" rel="footnote">3</a></sup></p>

<p>In this project, I used standard Principal Component Analysis (PCA). In the first step, I performed PCA on the original number of dimensions (i.e. features) and visualized the importance of each component with the scree plot:</p>

<figure style="width: 60%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-scree-plot.png" alt="PCA Scree Plot" />
</figure>

<p>As a good rule of thumb, one should keep as many components that together explain about 80% of variance. From the scree plot, one can see that the first 10 components in total capture almost 80% of the variance in the data. I wrote the code to automatically select this number of PCA components, so that I could easily test different datasets when clustering:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">starbucks_scaled</span><span class="p">)</span>
<span class="n">cum_expl_var_ratio</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>

<span class="c1">#choose number of components that explain ~80% of variance
</span><span class="n">components_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cum_expl_var_ratio</span><span class="p">[</span><span class="n">cum_expl_var_ratio</span> <span class="o">&lt;=</span> <span class="mf">0.805</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"number of pca components that explain 80%: </span><span class="si">{</span><span class="n">components_num</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">components_num</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">starbucks_scaled</span><span class="p">)</span>
<span class="n">starbucks_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">starbucks_scaled</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="5-model-implementation">5. Model Implementation</h1>

<h2 id="51-k-means-clustering">5.1. K-means clustering</h2>

<p>The goal of clustering is to identify groups of data, in which values are similar to each other. K-means is a popular algorithm in that it finds the data points that are closest to the cluster centroid. In technical terms, it tries to minimize the intra-cluster variation measured as the sum of the squared distances of each data point to the mean of the assigned cluster. <sup id="fnref:ft4" role="doc-noteref"><a href="#fn:ft4" class="footnote" rel="footnote">4</a></sup></p>

<p>Since the cluster means are assigned initially randomly, there is no guarantee that it will produce the optimal clustering result. It is usually recommended to run k-means algrorithm several times. Therefore, I set the number of iterations to 10.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Over a number of different cluster counts...
</span><span class="n">range_n_clusters</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">sum_of_squared_distances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">since</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">for</span> <span class="n">n_clusters</span> <span class="ow">in</span> <span class="n">range_n_clusters</span><span class="p">:</span>
    <span class="n">cluster_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"calculating {} clusters"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">))</span>
    <span class="c1"># run k-means clustering on the data and...
</span>    <span class="n">clusterer</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">starbucks_pca</span><span class="p">)</span>   
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">n_clusters</span><span class="si">}</span><span class="s">: silouhette score: </span><span class="si">{</span><span class="n">metrics</span><span class="p">.</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">starbucks_pca</span><span class="p">,</span> <span class="n">clusterer</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s">'euclidean'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="c1"># ... compute the average within-cluster distances.
</span>    <span class="n">sum_of_squared_distances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">clusterer</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"sum of squared distances:"</span><span class="p">,</span> <span class="n">clusterer</span><span class="p">.</span><span class="n">inertia_</span><span class="p">)</span>
    <span class="c1">#print("time for this cluster:", time.time() - cluster_start)
</span>
<span class="n">time_elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">since</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Clustering complete in {:.0f}m {:.0f}s'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">time_elapsed</span> <span class="o">//</span> <span class="mi">60</span><span class="p">,</span> <span class="n">time_elapsed</span> <span class="o">%</span> <span class="mi">60</span><span class="p">))</span>
</code></pre></div></div>
<p>When deciding upon the optimal clusters number, I relied on two heuristics - the elbow curve and silhouette score. The elbow curve plots the within-cluster sum of squared distances (WSS) as a function of number of clusters. As the number of clusters increase, there will be less and less data points in each cluster and hence they will be closer to their respective centroids, dropping WSS. One should choose a number of clusters so that adding another cluster doesn’t improve much better the total WSS. On the elbow curve, this will look like an elbow shape, hence the name.</p>

<p>Because elbow method is sometimes ambiguous, I used the average silouhette method to assist in deciding upon cluster number. 
The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The average silhouette method then computes the average silhouette value for all data points, which can range from -1 to +1 with values closer to 0 meaning the clusters overlap. The higher the score, the better the points in the cluster match their own cluster and worse the neighboring clusters.</p>

<figure style="width: 60%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-elbow-curve.png" alt="K-Means Elbow Curve" />
</figure>

<figure style="width: 60%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-silouhette-scores.png" alt="Silouhette Scores" />
</figure>

<p>From the elbow curve plot above, we see two potentially good cluster numbers - 3 and 5. Because the silhouette score is higher for 3 clusters than for 5 clusters and after checking the results visually I decided to keep 3 clusters. The 3 clusters are also aligned with the metrics better, as the clusters are formed around the offer types - bogo, discount and neither.</p>

<p>In general, however, the silhoutte scores are not particularly high - around 0.17, meaning that the clusters overlap and there is no clear separation. This is usual in marketing problems, where the boundaries between segments based on some people traits or behavior are rather vague. To see if there are any other better solutions, I tested different number of clusters along with other analytical steps discussed in the next section.</p>

<h2 id="52-refinement">5.2. Refinement</h2>

<p>When going through the full analytical cycle for the first time, I made the following decisions that predetermined the clustering results:</p>
<ul>
  <li>the decision to impute values</li>
  <li>the decision to use standardization when scaling</li>
  <li>the decision to perform PCA</li>
  <li>the decision to create 3 clusters</li>
</ul>

<p>In an attempt to diminish the influence of these decisions, I decided to refactor the code in form of functions that would allow quick experimentation. In particular, I was keen on checking the clustering results when:</p>
<ul>
  <li>missing values were dropped instead of imputed</li>
  <li>MinMax scaler was used instead of Standard Scaler</li>
  <li>no PCA was performed</li>
  <li>different number of clusters were specified</li>
</ul>

<h1 id="6-results">6. Results</h1>

<h2 id="61-model-evaluation">6.1. Model Evaluation</h2>

<p>To interpret clustering results, I first inversed the values that resulted from using Scaler and PCA:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">interpret_cluster</span><span class="p">(</span><span class="n">cluster_num</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">minmax</span><span class="p">):</span>
    <span class="s">'''
    Performs inversing the results of PCA and Scaler to allow cluster interpretation
    Input:
        cluster_num: number of clusters that were used to perform clustering
        df: data frame used for clustering
        minmax (bool): condition whetherminmax scaler was used 
    Output:
        results_df: data frame with inversed values for one cluster                
    '''</span>
    <span class="n">pca_inversed</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">clusterer</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="n">cluster_num</span><span class="p">,</span> <span class="p">:])</span>
    
    <span class="k">if</span> <span class="n">minmax</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">scaler_inversed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">around</span><span class="p">(</span><span class="n">scaler</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">pca_inversed</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler_inversed</span><span class="p">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scaler_inversed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">around</span><span class="p">(</span><span class="n">scaler</span><span class="p">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">pca_inversed</span><span class="p">),</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">results_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler_inversed</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">results_df</span>
</code></pre></div></div>

<p>With the help of another function, I created the final clustering results data frame and plotted its results against mean values (red line) for each feature:</p>

<figure style="width: 100%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-cluster-interpretation.png" alt="Starbucks Cluster Interpretation" />
</figure>

<p>Based on the above plots, I came to this clustering results:</p>

<p><strong>Cluster 1 - “Disinterested”:</strong>
<em>This group of customers are predominantly male that just recently became members. They tend to spend not much with below average number of transactions and small average order size. Although slightly more than 60% in this group view offers, they don’t complete them.</em></p>

<p><strong>Cluster 2 - “Discount-Type”:</strong>
<em>This group of customers are also mostly male but with the longest membership status (since 2013/2014). They tend to receive more discounts, which they love and actively complete. Their spending habits are slightly above average - they make small orders, but buy frequently.</em></p>

<p><strong>Cluster 3 - “Bogo-Type”:</strong>
<em>This is the only segment where female dominate over male. The customers in this group tend to be older and have higher income. They are loyal customers for few years already. They spend a lot - make huge orders and buy frequently. With such spending habits, no wonder that they are intersted in bogo and get rewarded the most. They complete bogo offers way beyond average, but also react to discounts from time to time.</em></p>

<h2 id="62-validation">6.2. Validation</h2>
<p>Since clustering techniques don’t have good metrics like supervised modeling to evaluate the modeling results, I relied on results validation through visualization of segments.</p>

<p>Here are the plots with the final results:</p>

<hr />
<p><strong>Segments Size</strong></p>

<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-clustering-segments.png" alt="Starbucks Clustering Segments" />
</figure>

<hr />
<p><strong>Metrics</strong></p>

<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-cluster-conversion-rates.png" alt="Starbucks Cluster Conversion Rates" />
</figure>

<p> </p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-response-rates.png" alt="Starbucks Response Rates" />
</figure>

<hr />

<p><strong>Profile</strong></p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-segments-age.png" alt="Starbucks Segments by Age" />
</figure>

<p> </p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-segments-income.png" alt="Starbucks Segments by Income" />
</figure>

<p> </p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-segments-gender.png" alt="Starbucks Segments by Gender" />
</figure>

<p> </p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-segments-membership.png" alt="Starbucks Segments by Membership Duration" />
</figure>

<hr />
<p><strong>Spending Habits</strong></p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-segments-spending.png" alt="Sturbucks Segments by Spending Amount" />
</figure>

<p> </p>
<figure style="width: 70%" class="align-center">
  <img src="http://localhost:4000/assets/images/posts/starbucks-segments-transactions.png" alt="Sturbucks Segments by Number of Transactions" />
</figure>

<h2 id="63-justification">6.3. Justification</h2>

<p>As noted in section 4.5, I tested several clustering versions - with MinMax/Standard Scaler, with/without PCA and on dataset with smaller features. In general, clustering with MinMax Scaler had different results from Standard Scaler, which were not really aligned with the metrics (i.e. differentiated between bogo/discount type worse). This is probably because I have a big number of numerical features with different scales, while MinMax works better for cases where the distribution is not Gaussian (e.g. categorical features).</p>

<p>In all other cases, the clustering results were good and differed mainly in the cluster sizes. 
As a result, I conclude that the initial decision to use Standard Scaler and perform PCA was correct.</p>

<h1 id="7-conclusion">7. Conclusion</h1>

<h2 id="71-reflection">7.1. Reflection</h2>
<p>Having segmented data into 3 segments, we now have better insight into Starbucks rewards user base. The clustering results would allow the company to better target audience with tailored offers in the next marketing campaign.</p>

<p>To arrive at this solution, I performed the full analysis cycle - cleaning and preprocessing the data, dealing with missing values, feature engineering, feature scaling, one hot encoding, dimensionality reduction and clustering. I also wrote a number of functions to generate the clustering results automatically, which allowed some quick experimentation.</p>

<p>While working on this project, I found that data preprocessing consumed a lot of time and was particularly challenging in this case because the event log didn’t account for particular marketing needs. As a marketer, for instance, I would not like when my response rates would be contaminated by offers viewed after the offer expiration date. Similarly, I would not like to waste marketing budget on customers that would buy the product even without promotional offer or earn rewards even when not viewing the offers. By imposing conditions on when offers should be considered as properly “viewed” or “completed”, I managed to fix this problem with original records. In the future, however, it would be advisable for Starbucks to implement a different tracking strategy as, for instance, by putting a reference code in the campaign message and asking the consumers to mention the code in order to receive the reward.</p>

<h2 id="72-improvement">7.2. Improvement</h2>
<p>One of the possible ways to improve the clustering results is to predict the missing age, income and gender values instead of simply imputing them. In this case, I would use the supervised machine learning, experimenting with different models like RandomForest, AdaBoost, etc. Another useful improvement would be to use the clustering results as labels in supervised modeling to actually predict customer’s probability of completing the offer. This would be a nice practical application that would allow extension on new customers and so would assist in executing a successful marketing campaign in the future.</p>

<!-- This is a capstone project for Udacity Data Science Nanodegree.  -->
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:ft1" role="doc-endnote">
      <p>The data is simulated but mimics the actual customer behavior closely. It is also a simplified version, since it contains information only about one product, while the real app sells a variaty of products. Finally, the data provided contains information on transactions/offer events during the 30 days trial test. <a href="#fnref:ft1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ft2" role="doc-endnote">
      <p>Farris, P., Bendle, N., Pfeifer, Ph., Reibstein, D. (2016): Marketing Metrics: The Manager’s Guide to Measuring Marketing Performance, 3rd Edition. Pearson Education, Inc. Chapter 8 “Promotions”, pp. 271-293. <a href="#fnref:ft2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ft3" role="doc-endnote">
      <p>cf. Ankur A. Patel (2019): “Hands-On Unsupervised Learning Using Python”. <a href="#fnref:ft3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ft4" role="doc-endnote">
      <p>cf. Peter Bruce, Andrew Bruce (2018): “Practical Statistics for Data Scientists”, O’Reilly. <a href="#fnref:ft4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
